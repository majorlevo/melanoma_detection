{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c2d258b",
   "metadata": {},
   "source": [
    "# Baseline Model Evaluation Report\n",
    "\n",
    "This notebook provides a comprehensive evaluation of the baseline ResNet18 model (`melanoma_model_weights.pth`) created in `3_create_first_model.ipynb`.\n",
    "\n",
    "## Evaluation Metrics:\n",
    "1. Overall Accuracy\n",
    "2. Per-class Precision, Recall, F1-Score\n",
    "3. **Melanoma Recall** (PRIMARY METRIC for medical applications)\n",
    "4. Confusion Matrix\n",
    "5. ROC Curves and AUC scores\n",
    "6. Classification Report\n",
    "7. Model architecture summary\n",
    "\n",
    "## Purpose:\n",
    "- Establish baseline performance for comparison with improved models\n",
    "- Identify weaknesses in baseline model\n",
    "- Document starting point before optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ba19899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825e2834",
   "metadata": {},
   "source": [
    "## 1. Load Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71cbc4c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\major\\Documents\\University\\DeepLearning\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\major\\Documents\\University\\DeepLearning\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Baseline model loaded from: melanoma_model_weights.pth\n",
      "\n",
      "Model Architecture: ResNet18\n",
      "Number of parameters: 11,178,051\n",
      "Trainable parameters: 11,178,051\n"
     ]
    }
   ],
   "source": [
    "# Load the baseline ResNet18 model\n",
    "model = models.resnet18(pretrained=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 3)  # 3 classes: benign, suspicious, melanoma\n",
    "\n",
    "model_path = \"melanoma_model_weights.pth\"\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(f\"‚úì Baseline model loaded from: {model_path}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Model file not found: {model_path}\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"\\nModel Architecture: ResNet18\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103a5d25",
   "metadata": {},
   "source": [
    "## 2. Prepare Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cee3b64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HAM10000Dataset(Dataset):\n",
    "    \"\"\"Dataset class for HAM10000.\"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, ann_dir, image_files, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.ann_dir = ann_dir\n",
    "        self.image_files = image_files\n",
    "        self.transform = transform\n",
    "\n",
    "        # Class mapping from original labels to 3-class groups\n",
    "        self.group_map = {\n",
    "            \"melanoma\": \"melanoma\",\n",
    "            \"basal cell carcinoma\": \"suspicious\",\n",
    "            \"actinic keratoses\": \"suspicious\",\n",
    "            \"melanocytic nevi\": \"benign\",\n",
    "            \"benign keratosis-like lesions\": \"benign\",\n",
    "            \"dermatofibroma\": \"benign\",\n",
    "            \"vascular lesions\": \"benign\",\n",
    "        }\n",
    "\n",
    "        self.group_to_idx = {\"benign\": 0, \"suspicious\": 1, \"melanoma\": 2}\n",
    "        self.idx_to_group = {0: \"benign\", 1: \"suspicious\", 2: \"melanoma\"}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load annotation\n",
    "        ann_path = os.path.join(self.ann_dir, img_name + \".json\")\n",
    "        try:\n",
    "            with open(ann_path, \"r\") as f:\n",
    "                ann = json.load(f)\n",
    "            original_label = ann[\"objects\"][0][\"classTitle\"]\n",
    "            group_label = self.group_map[original_label]\n",
    "            label = self.group_to_idx[group_label]\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {ann_path}: {e}\")\n",
    "            label = 0  # Default to benign\n",
    "\n",
    "        # Apply transforms\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c458c07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images found: 10015\n",
      "\n",
      "Dataset splits:\n",
      "  Training:   6409 images (64.0%)\n",
      "  Validation: 1603 images (16.0%)\n",
      "  Test:       2003 images (20.0%)\n",
      "\n",
      "‚úì Test dataset created with 2003 images\n"
     ]
    }
   ],
   "source": [
    "# Load test set files\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "image_dir = \"data/ham10000/ds/img\"\n",
    "ann_dir = \"data/ham10000/ds/ann\"\n",
    "\n",
    "# Get all image files\n",
    "all_files = [f for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "print(f\"Total images found: {len(all_files)}\")\n",
    "\n",
    "# Split into train/val/test (same as in other notebooks for consistency)\n",
    "train_files, test_files = train_test_split(all_files, test_size=0.2, random_state=42)\n",
    "train_files, val_files = train_test_split(train_files, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nDataset splits:\")\n",
    "print(f\"  Training:   {len(train_files)} images ({len(train_files)/len(all_files)*100:.1f}%)\")\n",
    "print(f\"  Validation: {len(val_files)} images ({len(val_files)/len(all_files)*100:.1f}%)\")\n",
    "print(f\"  Test:       {len(test_files)} images ({len(test_files)/len(all_files)*100:.1f}%)\")\n",
    "\n",
    "# Create test dataset (NO AUGMENTATION)\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_dataset = HAM10000Dataset(image_dir, ann_dir, test_files, transform=test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"\\n‚úì Test dataset created with {len(test_dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a54ed82",
   "metadata": {},
   "source": [
    "## 3. Run Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fdd4d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating baseline model on test set...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/63 [00:05<?, ?it/s]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 19976, 16676) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmpty\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\major\\Documents\\University\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1285\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1284\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\queues.py:114\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._poll():\n",
      "\u001b[31mEmpty\u001b[39m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluating baseline model on test set...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m preds, labels, probs = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Evaluation complete\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Predictions generated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(preds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, test_loader, device)\u001b[39m\n\u001b[32m      7\u001b[39m all_probs = []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mEvaluating\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\major\\Documents\\University\\DeepLearning\\.venv\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\major\\Documents\\University\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\major\\Documents\\University\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1492\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1489\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1491\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1492\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1495\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\major\\Documents\\University\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1454\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1450\u001b[39m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[32m   1451\u001b[39m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[32m   1452\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1453\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1454\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1455\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1456\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\major\\Documents\\University\\DeepLearning\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1298\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) > \u001b[32m0\u001b[39m:\n\u001b[32m   1297\u001b[39m     pids_str = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[38;5;28mstr\u001b[39m(w.pid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[32m-> \u001b[39m\u001b[32m1298\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1299\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) exited unexpectedly\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1300\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue.Empty):\n\u001b[32m   1302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: DataLoader worker (pid(s) 19976, 16676) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    \"\"\"Comprehensive model evaluation.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, batch_labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    return np.array(all_preds), np.array(all_labels), np.array(all_probs)\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "print(\"Evaluating baseline model on test set...\\n\")\n",
    "preds, labels, probs = evaluate_model(model, test_loader, device)\n",
    "\n",
    "print(f\"‚úì Evaluation complete\")\n",
    "print(f\"  Predictions generated: {len(preds)}\")\n",
    "print(f\"  Probability scores computed: {probs.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b4555",
   "metadata": {},
   "source": [
    "## 4. Overall Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7541da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"benign\", \"suspicious\", \"melanoma\"]\n",
    "\n",
    "# Calculate overall accuracy\n",
    "test_acc = accuracy_score(labels, preds)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"BASELINE MODEL PERFORMANCE REPORT\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nModel: ResNet18 (melanoma_model_weights.pth)\")\n",
    "print(f\"Test Set Size: {len(labels)} images\")\n",
    "print(f\"Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"OVERALL ACCURACY: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2459a9d0",
   "metadata": {},
   "source": [
    "## 5. Per-Class Performance (Clinical Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898de510",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLINICAL METRICS - PER CLASS PERFORMANCE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_preds_binary = (preds == i).astype(int)\n",
    "    class_labels_binary = (labels == i).astype(int)\n",
    "\n",
    "    recall = recall_score(class_labels_binary, class_preds_binary, zero_division=0)\n",
    "    precision = precision_score(class_labels_binary, class_preds_binary, zero_division=0)\n",
    "    f1 = f1_score(class_labels_binary, class_preds_binary, zero_division=0)\n",
    "    support = np.sum(labels == i)\n",
    "\n",
    "    print(f\"\\n{class_name.upper()}:\")\n",
    "    print(f\"  Support (# samples):  {support}\")\n",
    "    print(f\"  Precision:            {precision:.4f} ({precision*100:.2f}%)\")\n",
    "    print(\n",
    "        f\"  Recall (Sensitivity): {recall:.4f} ({recall*100:.2f}%) {'üî¥ CRITICAL!' if class_name == 'melanoma' else ''}\"\n",
    "    )\n",
    "    print(f\"  F1-Score:             {f1:.4f}\")\n",
    "\n",
    "# Calculate macro and weighted averages\n",
    "macro_precision = precision_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "macro_recall = recall_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "macro_f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "weighted_precision = precision_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "weighted_recall = recall_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "weighted_f1 = f1_score(labels, preds, average=\"weighted\", zero_division=0)\n",
    "\n",
    "# Melanoma-specific recall (PRIMARY METRIC)\n",
    "melanoma_recall = recall_score((labels == 2).astype(int), (preds == 2).astype(int), zero_division=0)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AGGREGATE METRICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nMacro Average (equal weight per class):\")\n",
    "print(f\"  Precision: {macro_precision:.4f}\")\n",
    "print(f\"  Recall:    {macro_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {macro_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nWeighted Average (by class support):\")\n",
    "print(f\"  Precision: {weighted_precision:.4f}\")\n",
    "print(f\"  Recall:    {weighted_recall:.4f}\")\n",
    "print(f\"  F1-Score:  {weighted_f1:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"üî¥ PRIMARY CLINICAL METRIC - MELANOMA RECALL: {melanoma_recall:.4f} ({melanoma_recall*100:.2f}%)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Clinical interpretation\n",
    "if melanoma_recall >= 0.95:\n",
    "    print(\"\\n‚úÖ EXCELLENT: Melanoma recall ‚â•95% - Clinically acceptable\")\n",
    "elif melanoma_recall >= 0.90:\n",
    "    print(\"\\n‚úì GOOD: Melanoma recall ‚â•90% - Acceptable with monitoring\")\n",
    "elif melanoma_recall >= 0.85:\n",
    "    print(\"\\n‚ö†Ô∏è WARNING: Melanoma recall <90% - Needs improvement\")\n",
    "else:\n",
    "    print(\"\\n‚ùå CRITICAL: Melanoma recall <85% - NOT clinically safe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c3e68e",
   "metadata": {},
   "source": [
    "## 6. Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c5d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "print(classification_report(labels, preds, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9455da",
   "metadata": {},
   "source": [
    "## 7. Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22be801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(labels, preds)\n",
    "\n",
    "print(\"\\nConfusion Matrix (Raw Counts):\")\n",
    "print(\"=\" * 70)\n",
    "print(\"              Predicted\")\n",
    "print(\"         Benign  Suspicious  Melanoma\")\n",
    "print(f\"Benign      {cm[0,0]:4d}      {cm[0,1]:4d}      {cm[0,2]:4d}\")\n",
    "print(f\"Suspicious  {cm[1,0]:4d}      {cm[1,1]:4d}      {cm[1,2]:4d}\")\n",
    "print(f\"Melanoma    {cm[2,0]:4d}      {cm[2,1]:4d}      {cm[2,2]:4d}\")\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={\"label\": \"Count\"},\n",
    ")\n",
    "plt.ylabel(\"True Label\", fontsize=12, fontweight=\"bold\")\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\"Confusion Matrix - Baseline ResNet18 Model\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"baseline_confusion_matrix.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Confusion matrix saved as: baseline_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01beac18",
   "metadata": {},
   "source": [
    "## 8. Normalized Confusion Matrix (Percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc4b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize confusion matrix by row (true labels)\n",
    "cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm_normalized,\n",
    "    annot=True,\n",
    "    fmt=\".2%\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={\"label\": \"Percentage\"},\n",
    ")\n",
    "plt.ylabel(\"True Label\", fontsize=12, fontweight=\"bold\")\n",
    "plt.xlabel(\"Predicted Label\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\"Normalized Confusion Matrix - Baseline ResNet18 Model\\n(Row percentages)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"baseline_confusion_matrix_normalized.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Normalized confusion matrix saved as: baseline_confusion_matrix_normalized.png\")\n",
    "\n",
    "# Highlight critical errors\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CRITICAL ERROR ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "melanoma_as_benign = cm[2, 0]  # True melanoma predicted as benign\n",
    "melanoma_total = cm[2, :].sum()\n",
    "print(f\"\\nüî¥ MOST DANGEROUS ERROR:\")\n",
    "print(\n",
    "    f\"   Melanoma misclassified as Benign: {melanoma_as_benign}/{melanoma_total} ({melanoma_as_benign/melanoma_total*100:.2f}%)\"\n",
    ")\n",
    "print(f\"   These are FALSE NEGATIVES - patient doesn't get treatment!\")\n",
    "\n",
    "melanoma_as_suspicious = cm[2, 1]  # True melanoma predicted as suspicious\n",
    "print(f\"\\n‚ö†Ô∏è MODERATE RISK:\")\n",
    "print(\n",
    "    f\"   Melanoma misclassified as Suspicious: {melanoma_as_suspicious}/{melanoma_total} ({melanoma_as_suspicious/melanoma_total*100:.2f}%)\"\n",
    ")\n",
    "print(f\"   These still get medical attention (better than benign)\")\n",
    "\n",
    "benign_as_melanoma = cm[0, 2]  # True benign predicted as melanoma\n",
    "benign_total = cm[0, :].sum()\n",
    "print(f\"\\n‚ö†Ô∏è FALSE ALARM (less critical):\")\n",
    "print(\n",
    "    f\"   Benign misclassified as Melanoma: {benign_as_melanoma}/{benign_total} ({benign_as_melanoma/benign_total*100:.2f}%)\"\n",
    ")\n",
    "print(f\"   These cause unnecessary anxiety but are safer than false negatives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae57e269",
   "metadata": {},
   "source": [
    "## 9. ROC Curves and AUC Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a59db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize labels for ROC curve (one-vs-rest)\n",
    "labels_binarized = label_binarize(labels, classes=[0, 1, 2])\n",
    "n_classes = 3\n",
    "\n",
    "# Compute ROC curve and AUC for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(labels_binarized[:, i], probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = [\"blue\", \"orange\", \"red\"]\n",
    "line_styles = [\"-\", \"--\", \"-.\"]\n",
    "\n",
    "for i, (color, ls) in enumerate(zip(colors, line_styles)):\n",
    "    plt.plot(\n",
    "        fpr[i], tpr[i], color=color, linestyle=ls, linewidth=2, label=f\"{class_names[i]} (AUC = {roc_auc[i]:.4f})\"\n",
    "    )\n",
    "\n",
    "plt.plot([0, 1], [0, 1], \"k--\", linewidth=1, label=\"Random Classifier (AUC = 0.5000)\")\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel(\"False Positive Rate\", fontsize=12, fontweight=\"bold\")\n",
    "plt.ylabel(\"True Positive Rate (Recall)\", fontsize=12, fontweight=\"bold\")\n",
    "plt.title(\"ROC Curves - Baseline ResNet18 Model\\n(One-vs-Rest)\", fontsize=14, fontweight=\"bold\")\n",
    "plt.legend(loc=\"lower right\", fontsize=11)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"baseline_roc_curves.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì ROC curves saved as: baseline_roc_curves.png\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AUC SCORES (Area Under ROC Curve)\")\n",
    "print(\"=\" * 70)\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"{class_name:12s}: {roc_auc[i]:.4f}\")\n",
    "\n",
    "# Macro average AUC\n",
    "macro_auc = np.mean(list(roc_auc.values()))\n",
    "print(f\"\\nMacro Average AUC: {macro_auc:.4f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  AUC = 1.0: Perfect classifier\")\n",
    "print(\"  AUC = 0.9-1.0: Excellent\")\n",
    "print(\"  AUC = 0.8-0.9: Good\")\n",
    "print(\"  AUC = 0.7-0.8: Fair\")\n",
    "print(\"  AUC = 0.5-0.7: Poor\")\n",
    "print(\"  AUC = 0.5: Random guess\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83312afc",
   "metadata": {},
   "source": [
    "## 10. Model Weaknesses and Improvement Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc53eb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BASELINE MODEL WEAKNESSES & IMPROVEMENT OPPORTUNITIES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Analyze performance gaps\n",
    "print(\"\\n1. MELANOMA DETECTION (Most Critical):\")\n",
    "if melanoma_recall < 0.90:\n",
    "    print(f\"   ‚ö†Ô∏è Melanoma recall ({melanoma_recall:.4f}) is below clinical threshold (0.90)\")\n",
    "    print(f\"   ‚Üí Need to increase sensitivity for melanoma class\")\n",
    "    print(f\"   ‚Üí Consider class weighting, focal loss, or ensemble methods\")\n",
    "else:\n",
    "    print(f\"   ‚úì Melanoma recall ({melanoma_recall:.4f}) meets clinical threshold\")\n",
    "\n",
    "print(\"\\n2. CLASS IMBALANCE:\")\n",
    "class_counts = [np.sum(labels == i) for i in range(3)]\n",
    "print(f\"   Benign:     {class_counts[0]} samples\")\n",
    "print(f\"   Suspicious: {class_counts[1]} samples\")\n",
    "print(f\"   Melanoma:   {class_counts[2]} samples\")\n",
    "imbalance_ratio = max(class_counts) / min(class_counts)\n",
    "print(f\"   Imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "if imbalance_ratio > 3:\n",
    "    print(f\"   ‚ö†Ô∏è Significant class imbalance detected\")\n",
    "    print(f\"   ‚Üí Use weighted loss function\")\n",
    "    print(f\"   ‚Üí Consider oversampling minority classes\")\n",
    "\n",
    "print(\"\\n3. MODEL ARCHITECTURE:\")\n",
    "print(f\"   Current: ResNet18 (~11M parameters)\")\n",
    "print(f\"   ‚Üí Try deeper models: ResNet50, EfficientNet, DenseNet\")\n",
    "print(f\"   ‚Üí Implement ensemble methods for robustness\")\n",
    "\n",
    "print(\"\\n4. DATA AUGMENTATION:\")\n",
    "print(f\"   Current: Unknown (check training notebook)\")\n",
    "print(f\"   ‚Üí Add rotation, flipping, color jitter\")\n",
    "print(f\"   ‚Üí Test different augmentation intensities\")\n",
    "\n",
    "print(\"\\n5. HYPERPARAMETER OPTIMIZATION:\")\n",
    "print(f\"   Current: Manual selection\")\n",
    "print(f\"   ‚Üí Use Optuna for systematic hyperparameter search\")\n",
    "print(f\"   ‚Üí Optimize learning rate, batch size, weight decay\")\n",
    "\n",
    "print(\"\\n6. OPTIMIZATION METRIC:\")\n",
    "print(f\"   Current: Likely accuracy-based\")\n",
    "print(f\"   ‚Üí Switch to recall-focused optimization\")\n",
    "print(f\"   ‚Üí Use composite score: 70% melanoma recall + 30% macro recall\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RECOMMENDED NEXT STEPS:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"1. Implement class-weighted loss function\")\n",
    "print(\"2. Add comprehensive data augmentation\")\n",
    "print(\"3. Try EfficientNet-B0 or ResNet50 architectures\")\n",
    "print(\"4. Run Optuna hyperparameter optimization\")\n",
    "print(\"5. Optimize for melanoma recall (not accuracy)\")\n",
    "print(\"6. Consider ensemble of top 3 models\")\n",
    "print(\"\\n‚Üí See 6_model_improvement.ipynb for implementation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f021c3d2",
   "metadata": {},
   "source": [
    "## 11. Save Evaluation Report to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_path = \"baseline_model_evaluation_report.txt\"\n",
    "\n",
    "with open(report_path, \"w\") as f:\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(\"BASELINE MODEL EVALUATION REPORT\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Model: ResNet18 (melanoma_model_weights.pth)\\n\")\n",
    "    f.write(f\"Test Set Size: {len(labels)} images\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "    # Overall metrics\n",
    "    f.write(\"OVERALL PERFORMANCE\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(f\"Overall Accuracy: {test_acc:.4f} ({test_acc*100:.2f}%)\\n\")\n",
    "    f.write(f\"Macro Precision:  {macro_precision:.4f}\\n\")\n",
    "    f.write(f\"Macro Recall:     {macro_recall:.4f}\\n\")\n",
    "    f.write(f\"Macro F1-Score:   {macro_f1:.4f}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "    # Per-class metrics\n",
    "    f.write(\"PER-CLASS PERFORMANCE\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        class_preds_binary = (preds == i).astype(int)\n",
    "        class_labels_binary = (labels == i).astype(int)\n",
    "        recall = recall_score(class_labels_binary, class_preds_binary, zero_division=0)\n",
    "        precision = precision_score(class_labels_binary, class_preds_binary, zero_division=0)\n",
    "        f1 = f1_score(class_labels_binary, class_preds_binary, zero_division=0)\n",
    "        support = np.sum(labels == i)\n",
    "\n",
    "        f.write(f\"\\n{class_name.upper()}:\\n\")\n",
    "        f.write(f\"  Support:   {support}\\n\")\n",
    "        f.write(f\"  Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"  Recall:    {recall:.4f}\\n\")\n",
    "        f.write(f\"  F1-Score:  {f1:.4f}\\n\")\n",
    "\n",
    "    # Primary metric\n",
    "    f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    f.write(f\"PRIMARY CLINICAL METRIC - MELANOMA RECALL: {melanoma_recall:.4f} ({melanoma_recall*100:.2f}%)\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "    # AUC scores\n",
    "    f.write(\"AUC SCORES\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        f.write(f\"{class_name:12s}: {roc_auc[i]:.4f}\\n\")\n",
    "    f.write(f\"\\nMacro Average: {macro_auc:.4f}\\n\\n\")\n",
    "\n",
    "    # Confusion matrix\n",
    "    f.write(\"CONFUSION MATRIX\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(\"              Predicted\\n\")\n",
    "    f.write(\"         Benign  Suspicious  Melanoma\\n\")\n",
    "    f.write(f\"Benign      {cm[0,0]:4d}      {cm[0,1]:4d}      {cm[0,2]:4d}\\n\")\n",
    "    f.write(f\"Suspicious  {cm[1,0]:4d}      {cm[1,1]:4d}      {cm[1,2]:4d}\\n\")\n",
    "    f.write(f\"Melanoma    {cm[2,0]:4d}      {cm[2,1]:4d}      {cm[2,2]:4d}\\n\\n\")\n",
    "\n",
    "    # Critical errors\n",
    "    f.write(\"CRITICAL ERROR ANALYSIS\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    f.write(\n",
    "        f\"Melanoma as Benign (FALSE NEGATIVE): {melanoma_as_benign}/{melanoma_total} ({melanoma_as_benign/melanoma_total*100:.2f}%)\\n\"\n",
    "    )\n",
    "    f.write(\n",
    "        f\"Melanoma as Suspicious: {melanoma_as_suspicious}/{melanoma_total} ({melanoma_as_suspicious/melanoma_total*100:.2f}%)\\n\"\n",
    "    )\n",
    "    f.write(\n",
    "        f\"Benign as Melanoma (FALSE POSITIVE): {benign_as_melanoma}/{benign_total} ({benign_as_melanoma/benign_total*100:.2f}%)\\n\\n\"\n",
    "    )\n",
    "\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "    f.write(\"END OF REPORT\\n\")\n",
    "    f.write(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "print(f\"\\n‚úì Evaluation report saved to: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66927476",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This evaluation provides a comprehensive baseline for your melanoma detection model. Key takeaways:\n",
    "\n",
    "1. **Primary Metric**: Melanoma recall is the most important for patient safety\n",
    "2. **False Negatives**: Most dangerous - missing melanoma cases\n",
    "3. **False Positives**: Less critical but cause unnecessary anxiety\n",
    "4. **Improvement Path**: See `6_model_improvement.ipynb` for optimization strategies\n",
    "\n",
    "### Files Generated:\n",
    "- `baseline_confusion_matrix.png`\n",
    "- `baseline_confusion_matrix_normalized.png`\n",
    "- `baseline_roc_curves.png`\n",
    "- `baseline_model_evaluation_report.txt`\n",
    "\n",
    "### Next Steps:\n",
    "Run `6_model_improvement.ipynb` to:\n",
    "- Optimize hyperparameters with Optuna\n",
    "- Test advanced architectures\n",
    "- Improve melanoma recall\n",
    "- Compare against this baseline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
