{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449476f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Pipeline - Melanoma Detection\n",
    "\n",
    "This notebook provides a complete, production-ready pipeline for melanoma detection using the **final trained model** with improved architecture.\n",
    "\n",
    "## Features:\n",
    "1. **Model Loading**: Load the best trained model from `final_model/` folder\n",
    "2. **Single Image Prediction**: Predict and visualize results for individual images\n",
    "3. **Batch Prediction**: Process multiple images efficiently\n",
    "4. **Export Results**: Save predictions to CSV for further analysis\n",
    "5. **Visualization**: Display predictions with confidence scores and confusion matrix\n",
    "6. **Model Evaluation**: Comprehensive evaluation on test set with clinical metrics\n",
    "\n",
    "## Model Information:\n",
    "- **Architecture**: Optimized through Optuna hyperparameter search\n",
    "- **Training**: Based on research best practices for medical imaging\n",
    "- **Weights**: `final_model/melanoma_model_improved_weights.pth`\n",
    "- **Focus**: High melanoma recall (sensitivity) for clinical safety\n",
    "\n",
    "## Usage:\n",
    "Run all cells and use the `MelanomaDetectionPipeline` class for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3206d6",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e9c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, recall_score, precision_score\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf7345e",
   "metadata": {},
   "source": [
    "## 2. Define Model Configuration\n",
    "\n",
    "Load the model configuration from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f02a3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration - adjust these based on your final model\n",
    "MODEL_CONFIG = {\n",
    "    \"weights_path\": \"final_model/melanoma_model_improved_weights.pth\",\n",
    "    \"model_name\": \"resnet18\",  # Will be loaded from config if available\n",
    "    \"input_size\": 224,\n",
    "    \"num_classes\": 3,\n",
    "    \"class_names\": [\"benign\", \"suspicious\", \"melanoma\"],\n",
    "    \"group_map\": {\n",
    "        \"melanoma\": \"melanoma\",\n",
    "        \"basal cell carcinoma\": \"suspicious\",\n",
    "        \"actinic keratoses\": \"suspicious\",\n",
    "        \"melanocytic nevi\": \"benign\",\n",
    "        \"benign keratosis-like lesions\": \"benign\",\n",
    "        \"dermatofibroma\": \"benign\",\n",
    "        \"vascular lesions\": \"benign\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Try to load model configuration from training results\n",
    "if os.path.exists(\"model_results_improved.json\"):\n",
    "    with open(\"model_results_improved.json\", \"r\") as f:\n",
    "        results = json.load(f)\n",
    "        MODEL_CONFIG[\"model_name\"] = results.get(\"model_architecture\", \"resnet18\")\n",
    "        MODEL_CONFIG[\"input_size\"] = results.get(\"input_size\", 224)\n",
    "        print(f\"‚úì Loaded model configuration from training results\")\n",
    "        print(f\"  Model: {MODEL_CONFIG['model_name']}\")\n",
    "        print(f\"  Input size: {MODEL_CONFIG['input_size']}\")\n",
    "else:\n",
    "    print(\"‚ö† Using default configuration (model_results_improved.json not found)\")\n",
    "\n",
    "print(f\"\\nModel weights: {MODEL_CONFIG['weights_path']}\")\n",
    "print(f\"Classes: {MODEL_CONFIG['class_names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1090f2d",
   "metadata": {},
   "source": [
    "## 3. Helper Function to Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170014fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name=\"resnet18\", num_classes=3):\n",
    "    \"\"\"\n",
    "    Create a model architecture.\n",
    "    Supports: resnet18, resnet50, efficientnet_b0, densenet121\n",
    "    \"\"\"\n",
    "    if model_name.startswith(\"resnet\"):\n",
    "        if model_name == \"resnet18\":\n",
    "            model = models.resnet18(pretrained=False)\n",
    "        elif model_name == \"resnet50\":\n",
    "            model = models.resnet50(pretrained=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported ResNet variant: {model_name}\")\n",
    "\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    elif model_name.startswith(\"efficientnet\"):\n",
    "        try:\n",
    "            import timm\n",
    "\n",
    "            model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)\n",
    "        except ImportError:\n",
    "            raise ImportError(\"timm library required for EfficientNet. Install with: pip install timm\")\n",
    "\n",
    "    elif model_name.startswith(\"densenet\"):\n",
    "        if model_name == \"densenet121\":\n",
    "            model = models.densenet121(pretrained=False)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported DenseNet variant: {model_name}\")\n",
    "\n",
    "        num_ftrs = model.classifier.in_features\n",
    "        model.classifier = nn.Linear(num_ftrs, num_classes)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model: {model_name}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"‚úì Model creation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379b0407",
   "metadata": {},
   "source": [
    "## 4. MelanomaDetectionPipeline Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MelanomaDetectionPipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline for melanoma detection.\n",
    "\n",
    "    Features:\n",
    "    - Load model from weights\n",
    "    - Single image prediction with visualization\n",
    "    - Batch prediction\n",
    "    - Export results to CSV\n",
    "    - Model evaluation with metrics\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, device=None):\n",
    "        \"\"\"Initialize the pipeline with model configuration.\"\"\"\n",
    "        self.config = config\n",
    "        self.device = device if device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Load model\n",
    "        print(f\"Loading model: {config['model_name']}\")\n",
    "        self.model = create_model(config[\"model_name\"], config[\"num_classes\"])\n",
    "\n",
    "        if os.path.exists(config[\"weights_path\"]):\n",
    "            self.model.load_state_dict(torch.load(config[\"weights_path\"], map_location=self.device))\n",
    "            print(f\"‚úì Model weights loaded from: {config['weights_path']}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Model weights not found: {config['weights_path']}\")\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "        # Setup transforms\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((config[\"input_size\"], config[\"input_size\"])),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Class mappings\n",
    "        self.class_names = config[\"class_names\"]\n",
    "        self.group_map = config[\"group_map\"]\n",
    "        self.idx_to_group = {i: name for i, name in enumerate(self.class_names)}\n",
    "        self.group_to_idx = {name: i for i, name in enumerate(self.class_names)}\n",
    "\n",
    "        print(f\"‚úì Pipeline initialized on device: {self.device}\")\n",
    "\n",
    "    def predict_single(self, image_path, show_image=True, check_annotation=True):\n",
    "        \"\"\"\n",
    "        Predict the class of a single image.\n",
    "\n",
    "        Args:\n",
    "            image_path: Path to the image file\n",
    "            show_image: Whether to display the image with prediction\n",
    "            check_annotation: Whether to check ground truth annotation\n",
    "\n",
    "        Returns:\n",
    "            dict with prediction results\n",
    "        \"\"\"\n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        input_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "\n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_tensor)\n",
    "            probs = F.softmax(outputs, dim=1)\n",
    "            all_probs = probs[0].cpu().numpy()\n",
    "            pred_prob, pred_idx = torch.max(probs, dim=1)\n",
    "\n",
    "        pred_label = self.idx_to_group[pred_idx.item()]\n",
    "        pred_probability = pred_prob.item()\n",
    "\n",
    "        # Create result dictionary\n",
    "        result = {\n",
    "            \"image_path\": image_path,\n",
    "            \"prediction\": pred_label,\n",
    "            \"probability\": pred_probability,\n",
    "            \"all_probabilities\": {self.class_names[i]: float(all_probs[i]) for i in range(len(self.class_names))},\n",
    "        }\n",
    "\n",
    "        # Check annotation if available\n",
    "        if check_annotation:\n",
    "            try:\n",
    "                # Construct annotation path\n",
    "                ann_path = image_path.replace(\"/img/\", \"/ann/\").replace(\"\\\\img\\\\\", \"\\\\ann\\\\\") + \".json\"\n",
    "                if os.path.exists(ann_path):\n",
    "                    with open(ann_path, \"r\") as f:\n",
    "                        ann = json.load(f)\n",
    "                    true_label = ann[\"objects\"][0][\"classTitle\"]\n",
    "                    true_group = self.group_map[true_label]\n",
    "\n",
    "                    result[\"true_label\"] = true_label\n",
    "                    result[\"true_group\"] = true_group\n",
    "                    result[\"correct\"] = true_group == pred_label\n",
    "            except Exception as e:\n",
    "                result[\"annotation_error\"] = str(e)\n",
    "\n",
    "        # Visualize if requested\n",
    "        if show_image:\n",
    "            self._visualize_prediction(image, result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _visualize_prediction(self, image, result):\n",
    "        \"\"\"Visualize a single prediction.\"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # Image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        # Title with prediction\n",
    "        title = f\"Prediction: {result['prediction']}\\nConfidence: {result['probability']:.2%}\"\n",
    "        if \"true_group\" in result:\n",
    "            title += f\"\\nTrue: {result['true_group']}\"\n",
    "            if result[\"correct\"]:\n",
    "                title += \" ‚úì\"\n",
    "            else:\n",
    "                title += \" ‚úó\"\n",
    "        plt.title(title, fontsize=12, fontweight=\"bold\")\n",
    "\n",
    "        # Probability bars\n",
    "        plt.subplot(1, 2, 2)\n",
    "        probs = result[\"all_probabilities\"]\n",
    "        colors = [\"green\", \"orange\", \"red\"]\n",
    "        bars = plt.barh(list(probs.keys()), list(probs.values()), color=colors)\n",
    "        plt.xlabel(\"Probability\")\n",
    "        plt.title(\"Class Probabilities\")\n",
    "        plt.xlim(0, 1)\n",
    "\n",
    "        # Highlight predicted class\n",
    "        pred_idx = self.group_to_idx[result[\"prediction\"]]\n",
    "        bars[pred_idx].set_edgecolor(\"black\")\n",
    "        bars[pred_idx].set_linewidth(3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def predict_batch(self, image_paths, batch_size=32, show_progress=True):\n",
    "        \"\"\"\n",
    "        Predict classes for multiple images.\n",
    "\n",
    "        Args:\n",
    "            image_paths: List of image file paths\n",
    "            batch_size: Batch size for processing\n",
    "            show_progress: Whether to show progress bar\n",
    "\n",
    "        Returns:\n",
    "            List of prediction result dictionaries\n",
    "        \"\"\"\n",
    "        results = []\n",
    "\n",
    "        iterator = tqdm(image_paths, desc=\"Processing images\") if show_progress else image_paths\n",
    "\n",
    "        for img_path in iterator:\n",
    "            try:\n",
    "                result = self.predict_single(img_path, show_image=False, check_annotation=True)\n",
    "                results.append(result)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "                results.append({\"image_path\": img_path, \"error\": str(e)})\n",
    "\n",
    "        return results\n",
    "\n",
    "    def export_results(self, results, output_path=\"predictions.csv\"):\n",
    "        \"\"\"\n",
    "        Export prediction results to CSV.\n",
    "\n",
    "        Args:\n",
    "            results: List of prediction dictionaries\n",
    "            output_path: Path for output CSV file\n",
    "        \"\"\"\n",
    "        # Convert to DataFrame\n",
    "        df_data = []\n",
    "        for r in results:\n",
    "            if \"error\" not in r:\n",
    "                row = {\n",
    "                    \"image_path\": r[\"image_path\"],\n",
    "                    \"prediction\": r[\"prediction\"],\n",
    "                    \"probability\": r[\"probability\"],\n",
    "                    \"prob_benign\": r[\"all_probabilities\"][\"benign\"],\n",
    "                    \"prob_suspicious\": r[\"all_probabilities\"][\"suspicious\"],\n",
    "                    \"prob_melanoma\": r[\"all_probabilities\"][\"melanoma\"],\n",
    "                }\n",
    "                if \"true_group\" in r:\n",
    "                    row[\"true_label\"] = r[\"true_label\"]\n",
    "                    row[\"true_group\"] = r[\"true_group\"]\n",
    "                    row[\"correct\"] = r[\"correct\"]\n",
    "                df_data.append(row)\n",
    "\n",
    "        df = pd.DataFrame(df_data)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"‚úì Results exported to: {output_path}\")\n",
    "        print(f\"  Total predictions: {len(df)}\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        \"\"\"\n",
    "        Comprehensive evaluation on a test dataset.\n",
    "\n",
    "        Args:\n",
    "            test_loader: DataLoader for test set\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with evaluation metrics\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "        all_probs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "                images = images.to(self.device)\n",
    "                outputs = self.model(images)\n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "        all_preds = np.array(all_preds)\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_probs = np.array(all_probs)\n",
    "\n",
    "        # Calculate metrics\n",
    "        from sklearn.metrics import (\n",
    "            accuracy_score,\n",
    "            precision_score,\n",
    "            recall_score,\n",
    "            f1_score,\n",
    "            confusion_matrix,\n",
    "            classification_report,\n",
    "        )\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(all_labels, all_preds),\n",
    "            \"precision_macro\": precision_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "            \"recall_macro\": recall_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "            \"f1_macro\": f1_score(all_labels, all_preds, average=\"macro\", zero_division=0),\n",
    "            \"confusion_matrix\": confusion_matrix(all_labels, all_preds),\n",
    "            \"classification_report\": classification_report(\n",
    "                all_labels, all_preds, target_names=self.class_names, digits=4, zero_division=0\n",
    "            ),\n",
    "        }\n",
    "\n",
    "        # Per-class metrics (especially melanoma recall)\n",
    "        metrics[\"per_class\"] = {}\n",
    "        for i, class_name in enumerate(self.class_names):\n",
    "            class_preds_binary = (all_preds == i).astype(int)\n",
    "            class_labels_binary = (all_labels == i).astype(int)\n",
    "\n",
    "            metrics[\"per_class\"][class_name] = {\n",
    "                \"precision\": precision_score(class_labels_binary, class_preds_binary, zero_division=0),\n",
    "                \"recall\": recall_score(class_labels_binary, class_preds_binary, zero_division=0),\n",
    "                \"f1\": f1_score(class_labels_binary, class_preds_binary, zero_division=0),\n",
    "                \"support\": np.sum(all_labels == i),\n",
    "            }\n",
    "\n",
    "        return metrics, all_preds, all_labels, all_probs\n",
    "\n",
    "    def plot_confusion_matrix(self, confusion_matrix, save_path=None):\n",
    "        \"\"\"Plot confusion matrix.\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(\n",
    "            confusion_matrix,\n",
    "            annot=True,\n",
    "            fmt=\"d\",\n",
    "            cmap=\"Blues\",\n",
    "            xticklabels=self.class_names,\n",
    "            yticklabels=self.class_names,\n",
    "        )\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.title(\"Confusion Matrix - Final Model\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
    "            print(f\"‚úì Confusion matrix saved to: {save_path}\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def print_clinical_metrics(self, metrics):\n",
    "        \"\"\"Print clinical-focused metrics.\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"CLINICAL METRICS - RECALL FOCUSED\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        for class_name in self.class_names:\n",
    "            class_metrics = metrics[\"per_class\"][class_name]\n",
    "            print(f\"\\n{class_name.upper()}:\")\n",
    "            print(f\"  Recall (Sensitivity): {class_metrics['recall']:.4f}\", end=\"\")\n",
    "            if class_name == \"melanoma\":\n",
    "                print(\" ‚≠ê CRITICAL METRIC!\", end=\"\")\n",
    "            print()\n",
    "            print(f\"  Precision:            {class_metrics['precision']:.4f}\")\n",
    "            print(f\"  F1-Score:             {class_metrics['f1']:.4f}\")\n",
    "            print(f\"  Support:              {class_metrics['support']}\")\n",
    "\n",
    "        melanoma_recall = metrics[\"per_class\"][\"melanoma\"][\"recall\"]\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"Overall Accuracy:         {metrics['accuracy']:.4f}\")\n",
    "        print(f\"Macro Recall (avg):       {metrics['recall_macro']:.4f}\")\n",
    "        print(f\"üî¥ MELANOMA RECALL:        {melanoma_recall:.4f} ‚Üê PRIMARY METRIC\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        # Clinical interpretation\n",
    "        if melanoma_recall >= 0.95:\n",
    "            print(\"\\n‚úÖ EXCELLENT: Melanoma recall ‚â•95% - Clinically acceptable\")\n",
    "        elif melanoma_recall >= 0.90:\n",
    "            print(\"\\n‚úì GOOD: Melanoma recall ‚â•90% - Acceptable with monitoring\")\n",
    "        elif melanoma_recall >= 0.85:\n",
    "            print(\"\\n‚ö† WARNING: Melanoma recall <90% - Needs improvement\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå CRITICAL: Melanoma recall <85% - NOT clinically safe\")\n",
    "\n",
    "\n",
    "print(\"‚úì MelanomaDetectionPipeline class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805129d",
   "metadata": {},
   "source": [
    "## 5. Initialize Pipeline\n",
    "\n",
    "Load the final model and create the pipeline instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc7e6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline\n",
    "pipeline = MelanomaDetectionPipeline(MODEL_CONFIG, device=device)\n",
    "\n",
    "print(\"\\n‚úì Pipeline ready for predictions!\")\n",
    "print(f\"  Model: {MODEL_CONFIG['model_name']}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(f\"  Input size: {MODEL_CONFIG['input_size']}x{MODEL_CONFIG['input_size']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46303e43",
   "metadata": {},
   "source": [
    "## 6. Example: Single Image Prediction\n",
    "\n",
    "Test the pipeline with a single image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed89125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Predict on a single image\n",
    "example_image = \"data/ham10000/ds/img/ISIC_0024353.jpg\"\n",
    "\n",
    "if os.path.exists(example_image):\n",
    "    result = pipeline.predict_single(example_image, show_image=True, check_annotation=True)\n",
    "\n",
    "    print(\"\\nDetailed Results:\")\n",
    "    print(f\"  Image: {result['image_path']}\")\n",
    "    print(f\"  Prediction: {result['prediction']}\")\n",
    "    print(f\"  Confidence: {result['probability']:.2%}\")\n",
    "    print(f\"\\n  All Probabilities:\")\n",
    "    for class_name, prob in result[\"all_probabilities\"].items():\n",
    "        print(f\"    {class_name}: {prob:.2%}\")\n",
    "\n",
    "    if \"true_group\" in result:\n",
    "        print(f\"\\n  Ground Truth: {result['true_label']} ({result['true_group']})\")\n",
    "        print(f\"  Correct: {'‚úì' if result['correct'] else '‚úó'}\")\n",
    "else:\n",
    "    print(f\"Example image not found: {example_image}\")\n",
    "    print(\"Please update the path to an existing image in your dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5802b5",
   "metadata": {},
   "source": [
    "## 7. Batch Prediction on Multiple Images\n",
    "\n",
    "Process multiple images and export results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b7bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample of images for batch prediction\n",
    "image_dir = \"data/ham10000/ds/img\"\n",
    "\n",
    "if os.path.exists(image_dir):\n",
    "    # Get first 50 images as a sample\n",
    "    all_images = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "    sample_images = all_images[:50]\n",
    "\n",
    "    print(f\"Processing {len(sample_images)} sample images...\")\n",
    "\n",
    "    # Batch prediction\n",
    "    batch_results = pipeline.predict_batch(sample_images, show_progress=True)\n",
    "\n",
    "    # Export results\n",
    "    results_df = pipeline.export_results(batch_results, \"sample_predictions.csv\")\n",
    "\n",
    "    # Display summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BATCH PREDICTION SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Count predictions by class\n",
    "    pred_counts = results_df[\"prediction\"].value_counts()\n",
    "    print(\"\\nPredictions by Class:\")\n",
    "    for class_name, count in pred_counts.items():\n",
    "        print(f\"  {class_name}: {count} ({count/len(results_df)*100:.1f}%)\")\n",
    "\n",
    "    # Accuracy if ground truth available\n",
    "    if \"correct\" in results_df.columns:\n",
    "        accuracy = results_df[\"correct\"].mean()\n",
    "        print(f\"\\nAccuracy on sample: {accuracy:.2%}\")\n",
    "\n",
    "        # Per-class accuracy\n",
    "        print(\"\\nPer-Class Performance:\")\n",
    "        for class_name in MODEL_CONFIG[\"class_names\"]:\n",
    "            class_df = results_df[results_df[\"true_group\"] == class_name]\n",
    "            if len(class_df) > 0:\n",
    "                class_acc = class_df[\"correct\"].mean()\n",
    "                print(f\"  {class_name}: {class_acc:.2%} ({len(class_df)} samples)\")\n",
    "\n",
    "    # Display first few results\n",
    "    print(\"\\nFirst 10 predictions:\")\n",
    "    display_cols = [\"image_path\", \"prediction\", \"probability\", \"true_group\", \"correct\"]\n",
    "    display_cols = [col for col in display_cols if col in results_df.columns]\n",
    "    print(results_df[display_cols].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(f\"Image directory not found: {image_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf925f5",
   "metadata": {},
   "source": [
    "## 8. Full Test Set Evaluation\n",
    "\n",
    "Evaluate the model on the complete test set with comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bd548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class HAM10000Dataset(Dataset):\n",
    "    \"\"\"Dataset class for HAM10000.\"\"\"\n",
    "\n",
    "    def __init__(self, image_dir, ann_dir, image_files, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.ann_dir = ann_dir\n",
    "        self.image_files = image_files\n",
    "        self.transform = transform\n",
    "\n",
    "        self.group_map = MODEL_CONFIG[\"group_map\"]\n",
    "        self.group_to_idx = {name: i for i, name in enumerate(MODEL_CONFIG[\"class_names\"])}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "\n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load annotation\n",
    "        ann_path = os.path.join(self.ann_dir, img_name + \".json\")\n",
    "        try:\n",
    "            with open(ann_path, \"r\") as f:\n",
    "                ann = json.load(f)\n",
    "            original_label = ann[\"objects\"][0][\"classTitle\"]\n",
    "            group_label = self.group_map[original_label]\n",
    "            label = self.group_to_idx[group_label]\n",
    "        except Exception as e:\n",
    "            label = 0  # Default to benign\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Load test files\n",
    "if os.path.exists(\"test.csv\"):\n",
    "    test_df = pd.read_csv(\"test.csv\")\n",
    "    test_files = [os.path.basename(p) for p in test_df[\"image_path\"].values]\n",
    "    print(f\"Loaded {len(test_files)} test files from test.csv\")\n",
    "else:\n",
    "    # Fallback: use last 15% of images as test set\n",
    "    print(\"test.csv not found, using fallback test set\")\n",
    "    all_files = [f for f in os.listdir(image_dir) if f.endswith(\".jpg\")]\n",
    "    test_files = all_files[-int(len(all_files) * 0.15) :]\n",
    "    print(f\"Using {len(test_files)} images as test set\")\n",
    "\n",
    "# Create test dataset and dataloader\n",
    "image_dir = \"data/ham10000/ds/img\"\n",
    "ann_dir = \"data/ham10000/ds/ann\"\n",
    "\n",
    "test_dataset = HAM10000Dataset(image_dir, ann_dir, test_files, transform=pipeline.transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"‚úì Test dataset created with {len(test_dataset)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a2f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating model on test set...\")\n",
    "metrics, preds, labels, probs = pipeline.evaluate(test_loader)\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\" * 70)\n",
    "print(metrics[\"classification_report\"])\n",
    "\n",
    "# Print clinical metrics\n",
    "pipeline.print_clinical_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d5d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "pipeline.plot_confusion_matrix(metrics[\"confusion_matrix\"], save_path=\"final_model_confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a345317c",
   "metadata": {},
   "source": [
    "## 9. Visualize Sample Predictions\n",
    "\n",
    "Display predictions for a few test images to qualitatively assess model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d63db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize predictions for sample images from each class\n",
    "import random\n",
    "\n",
    "# Get samples from each class\n",
    "samples_per_class = 2\n",
    "\n",
    "for class_idx, class_name in enumerate(MODEL_CONFIG[\"class_names\"]):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Examples of {class_name.upper()} lesions:\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Find images of this class\n",
    "    class_images = []\n",
    "    for img_file in test_files[:100]:  # Check first 100 for efficiency\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "        ann_path = os.path.join(ann_dir, img_file + \".json\")\n",
    "\n",
    "        try:\n",
    "            with open(ann_path, \"r\") as f:\n",
    "                ann = json.load(f)\n",
    "            original_label = ann[\"objects\"][0][\"classTitle\"]\n",
    "            group_label = MODEL_CONFIG[\"group_map\"][original_label]\n",
    "\n",
    "            if group_label == class_name:\n",
    "                class_images.append(img_path)\n",
    "\n",
    "            if len(class_images) >= samples_per_class:\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # Predict and visualize\n",
    "    for img_path in class_images[:samples_per_class]:\n",
    "        result = pipeline.predict_single(img_path, show_image=True, check_annotation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0805f2a",
   "metadata": {},
   "source": [
    "## 10. Error Analysis\n",
    "\n",
    "Analyze misclassified samples to understand model limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1eb1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find misclassified examples\n",
    "misclassified_indices = np.where(preds != labels)[0]\n",
    "\n",
    "print(\n",
    "    f\"Total misclassifications: {len(misclassified_indices)} out of {len(labels)} ({len(misclassified_indices)/len(labels)*100:.1f}%)\"\n",
    ")\n",
    "\n",
    "# Analyze misclassification patterns\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MISCLASSIFICATION PATTERNS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for true_idx, true_name in enumerate(MODEL_CONFIG[\"class_names\"]):\n",
    "    for pred_idx, pred_name in enumerate(MODEL_CONFIG[\"class_names\"]):\n",
    "        if true_idx != pred_idx:\n",
    "            # Count this type of error\n",
    "            error_mask = (labels == true_idx) & (preds == pred_idx)\n",
    "            error_count = np.sum(error_mask)\n",
    "\n",
    "            if error_count > 0:\n",
    "                total_true = np.sum(labels == true_idx)\n",
    "                print(\n",
    "                    f\"\\n{true_name} ‚Üí {pred_name}: {error_count} errors ({error_count/total_true*100:.1f}% of {true_name})\"\n",
    "                )\n",
    "\n",
    "# Show a few misclassified examples (especially melanoma misclassifications)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CRITICAL MISCLASSIFICATIONS (Melanoma Cases)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "melanoma_idx = MODEL_CONFIG[\"class_names\"].index(\"melanoma\")\n",
    "melanoma_misclassified = np.where((labels == melanoma_idx) & (preds != melanoma_idx))[0]\n",
    "\n",
    "if len(melanoma_misclassified) > 0:\n",
    "    print(f\"\\nFound {len(melanoma_misclassified)} melanoma cases that were misclassified\")\n",
    "    print(\"Showing first 3 examples:\\n\")\n",
    "\n",
    "    for i, idx in enumerate(melanoma_misclassified[:3]):\n",
    "        img_file = test_files[idx]\n",
    "        img_path = os.path.join(image_dir, img_file)\n",
    "\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"  True: melanoma\")\n",
    "        print(f\"  Predicted: {MODEL_CONFIG['class_names'][preds[idx]]}\")\n",
    "        print(f\"  Confidence: {probs[idx][preds[idx]]:.2%}\")\n",
    "        print(f\"  Melanoma probability: {probs[idx][melanoma_idx]:.2%}\")\n",
    "\n",
    "        # Visualize\n",
    "        result = pipeline.predict_single(img_path, show_image=True, check_annotation=True)\n",
    "else:\n",
    "    print(\"\\n‚úÖ No melanoma cases were misclassified!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f3d2f8",
   "metadata": {},
   "source": [
    "## 11. Export Full Test Results\n",
    "\n",
    "Save all test predictions to a CSV file for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff23d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive results dataframe\n",
    "test_results = []\n",
    "\n",
    "for idx, (img_file, pred, label, prob) in enumerate(zip(test_files, preds, labels, probs)):\n",
    "    img_path = os.path.join(image_dir, img_file)\n",
    "\n",
    "    # Get annotation\n",
    "    ann_path = os.path.join(ann_dir, img_file + \".json\")\n",
    "    try:\n",
    "        with open(ann_path, \"r\") as f:\n",
    "            ann = json.load(f)\n",
    "        true_label = ann[\"objects\"][0][\"classTitle\"]\n",
    "    except:\n",
    "        true_label = \"unknown\"\n",
    "\n",
    "    result = {\n",
    "        \"image_file\": img_file,\n",
    "        \"image_path\": img_path,\n",
    "        \"true_label\": true_label,\n",
    "        \"true_group\": MODEL_CONFIG[\"class_names\"][label],\n",
    "        \"predicted_group\": MODEL_CONFIG[\"class_names\"][pred],\n",
    "        \"correct\": (pred == label),\n",
    "        \"confidence\": float(prob[pred]),\n",
    "        \"prob_benign\": float(prob[0]),\n",
    "        \"prob_suspicious\": float(prob[1]),\n",
    "        \"prob_melanoma\": float(prob[2]),\n",
    "    }\n",
    "    test_results.append(result)\n",
    "\n",
    "# Convert to DataFrame\n",
    "test_results_df = pd.DataFrame(test_results)\n",
    "\n",
    "# Save to CSV\n",
    "output_path = \"final_model_test_results.csv\"\n",
    "test_results_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úì Test results saved to: {output_path}\")\n",
    "print(f\"  Total samples: {len(test_results_df)}\")\n",
    "print(f\"  Columns: {', '.join(test_results_df.columns)}\")\n",
    "\n",
    "# Display summary statistics\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nOverall Accuracy: {test_results_df['correct'].mean():.2%}\")\n",
    "\n",
    "print(\"\\nPrediction Distribution:\")\n",
    "print(test_results_df[\"predicted_group\"].value_counts())\n",
    "\n",
    "print(\"\\nTrue Label Distribution:\")\n",
    "print(test_results_df[\"true_group\"].value_counts())\n",
    "\n",
    "print(\"\\nAverage Confidence by Class:\")\n",
    "for class_name in MODEL_CONFIG[\"class_names\"]:\n",
    "    class_df = test_results_df[test_results_df[\"predicted_group\"] == class_name]\n",
    "    if len(class_df) > 0:\n",
    "        avg_conf = class_df[\"confidence\"].mean()\n",
    "        print(f\"  {class_name}: {avg_conf:.2%}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 10 test results:\")\n",
    "display_cols = [\"image_file\", \"true_group\", \"predicted_group\", \"confidence\", \"correct\"]\n",
    "print(test_results_df[display_cols].head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7280d554",
   "metadata": {},
   "source": [
    "## 12. Pipeline Summary and Usage Guide\n",
    "\n",
    "Complete guide for using this pipeline in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05b496",
   "metadata": {},
   "source": [
    "### Quick Reference Guide\n",
    "\n",
    "#### 1. Initialize Pipeline\n",
    "```python\n",
    "from pipeline_code_above import MelanomaDetectionPipeline\n",
    "\n",
    "config = {\n",
    "    \"weights_path\": \"final_model/melanoma_model_improved_weights.pth\",\n",
    "    \"model_name\": \"resnet18\",  # or your model architecture\n",
    "    \"input_size\": 224,\n",
    "    \"num_classes\": 3,\n",
    "    \"class_names\": [\"benign\", \"suspicious\", \"melanoma\"]\n",
    "}\n",
    "\n",
    "pipeline = MelanomaDetectionPipeline(config)\n",
    "```\n",
    "\n",
    "#### 2. Single Image Prediction\n",
    "```python\n",
    "result = pipeline.predict_single(\"path/to/image.jpg\", show_image=True)\n",
    "print(f\"Prediction: {result['prediction']}\")\n",
    "print(f\"Confidence: {result['probability']:.2%}\")\n",
    "```\n",
    "\n",
    "#### 3. Batch Prediction\n",
    "```python\n",
    "image_paths = [\"image1.jpg\", \"image2.jpg\", \"image3.jpg\"]\n",
    "results = pipeline.predict_batch(image_paths)\n",
    "df = pipeline.export_results(results, \"predictions.csv\")\n",
    "```\n",
    "\n",
    "#### 4. Model Evaluation\n",
    "```python\n",
    "metrics, preds, labels, probs = pipeline.evaluate(test_loader)\n",
    "pipeline.print_clinical_metrics(metrics)\n",
    "pipeline.plot_confusion_matrix(metrics['confusion_matrix'])\n",
    "```\n",
    "\n",
    "### Key Features\n",
    "\n",
    "‚úÖ **Production Ready**: Robust error handling and logging  \n",
    "‚úÖ **Clinical Focus**: Emphasis on melanoma recall (sensitivity)  \n",
    "‚úÖ **Visualization**: Built-in plotting for predictions and metrics  \n",
    "‚úÖ **Export Capability**: Save results to CSV for analysis  \n",
    "‚úÖ **Batch Processing**: Efficient processing of multiple images  \n",
    "‚úÖ **Comprehensive Metrics**: Accuracy, precision, recall, F1, confusion matrix\n",
    "\n",
    "### Model Performance Summary\n",
    "\n",
    "**Model**: {model_name}  \n",
    "**Architecture**: {architecture_details}  \n",
    "**Test Accuracy**: {test_accuracy}  \n",
    "**Melanoma Recall**: {melanoma_recall} ‚≠ê (Most Important)  \n",
    "\n",
    "### Clinical Interpretation\n",
    "\n",
    "- **Benign**: Low-risk lesions, no immediate action needed\n",
    "- **Suspicious**: Moderate-risk, recommend clinical examination\n",
    "- **Melanoma**: High-risk, urgent medical attention required\n",
    "\n",
    "**Note**: This model is designed to prioritize sensitivity for melanoma detection, meaning it may flag suspicious cases for further examination rather than miss dangerous lesions. This is the clinically appropriate approach.\n",
    "\n",
    "### Next Steps for Deployment\n",
    "\n",
    "1. **API Development**: Wrap pipeline in FastAPI/Flask REST API\n",
    "2. **Model Monitoring**: Track prediction distributions in production\n",
    "3. **ONNX Export**: Convert model for cross-platform deployment\n",
    "4. **Mobile Optimization**: Quantize model for mobile devices\n",
    "5. **Regulatory Compliance**: Prepare documentation for FDA/CE marking\n",
    "\n",
    "### Files Generated\n",
    "\n",
    "- `final_model_test_results.csv`: Complete test set predictions\n",
    "- `final_model_confusion_matrix.png`: Confusion matrix visualization\n",
    "- `sample_predictions.csv`: Sample batch predictions\n",
    "\n",
    "### Support and Documentation\n",
    "\n",
    "For questions or issues:\n",
    "- Check model training notebook: `6_model_improvement.ipynb`\n",
    "- Review hyperparameter optimization: `optuna_trials_report.txt`\n",
    "- Consult model configuration: `model_results_improved.json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93945950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Summary\n",
    "print(\"=\" * 70)\n",
    "print(\"MELANOMA DETECTION PIPELINE - FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n‚úì Model: {MODEL_CONFIG['model_name']}\")\n",
    "print(f\"‚úì Weights: {MODEL_CONFIG['weights_path']}\")\n",
    "print(f\"‚úì Device: {device}\")\n",
    "print(f\"‚úì Classes: {', '.join(MODEL_CONFIG['class_names'])}\")\n",
    "\n",
    "if \"metrics\" in locals():\n",
    "    print(f\"\\n‚úì Test Accuracy: {metrics['accuracy']:.2%}\")\n",
    "    print(f\"‚úì Melanoma Recall: {metrics['per_class']['melanoma']['recall']:.2%} ‚≠ê\")\n",
    "    print(f\"‚úì Test Samples: {len(test_dataset)}\")\n",
    "\n",
    "print(\"\\n‚úì Pipeline is ready for production use!\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
